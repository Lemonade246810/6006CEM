# -*- coding: utf-8 -*-
"""Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ePDBzW87iGgXEBJJYL3e-tu83gOe2MXA
"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder

# Load the dataset
url = "flight_delays.csv"
df = pd.read_csv(url)
df

# Step 1: Data Preprocessing
# Handling missing values by dropping rows where 'DelayMinutes' is null since it's the target variable
df_clean = df.dropna(subset=['DelayMinutes'])

# Filling missing values for DelayReason
df_clean['DelayReason'].fillna('Unknown', inplace=True)

# Convert time-related columns like 'ScheduledDeparture', 'ActualDeparture' into datetime format
df_clean['ScheduledDeparture'] = pd.to_datetime(df_clean['ScheduledDeparture'], errors='coerce')
df_clean['ActualDeparture'] = pd.to_datetime(df_clean['ActualDeparture'], errors='coerce')
df_clean['ScheduledArrival'] = pd.to_datetime(df_clean['ScheduledArrival'], errors='coerce')
df_clean['ActualArrival'] = pd.to_datetime(df_clean['ActualArrival'], errors='coerce')

# Feature engineering: Create new time-related features like the delay in departure/arrival
df_clean['DepartureDelay'] = (df_clean['ActualDeparture'] - df_clean['ScheduledDeparture']).dt.total_seconds() / 60.0
df_clean['ArrivalDelay'] = (df_clean['ActualArrival'] - df_clean['ScheduledArrival']).dt.total_seconds() / 60.0

# Drop original time columns (if not needed) and non-predictive columns
df_clean = df_clean.drop(columns=['ScheduledDeparture', 'ActualDeparture', 'ScheduledArrival', 'ActualArrival',
                                  'FlightID', 'FlightNumber', 'TailNumber'])

# Convert categorical variables to numeric via Label Encoding or One-Hot Encoding
label_cols = ['Airline', 'Origin', 'Destination', 'DelayReason', 'AircraftType', 'Cancelled', 'Diverted']

# For simplicity, use LabelEncoder for categorical variables
label_encoders = {}
for col in label_cols:
    # Ensure the column contains only strings before applying LabelEncoder
    # by converting boolean values to their string representations.
    df_clean[col] = df_clean[col].astype(str)
    le = LabelEncoder()
    df_clean[col] = le.fit_transform(df_clean[col])
    label_encoders[col] = le  # Save the label encoder for inverse_transform if needed

# Step 2: Define Features and Target
X = df_clean.drop(columns=['DelayMinutes'])  # Features
y = df_clean['DelayMinutes']  # Target (delay in minutes)

# Step 3: Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 4: Train Random Forest Model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Step 5: Predict and Evaluate Random Forest Model
y_pred_rf = rf_model.predict(X_test)
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Random Forest Regression MSE: {mse_rf}")
print(f"Random Forest Regression R2 Score: {r2_rf}")

# Step 6: Train XGBoost Model
xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train, y_train)

# Predict and Evaluate XGBoost Model
y_pred_xgb = xgb_model.predict(X_test)
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f"XGBoost Regression MSE: {mse_xgb}")
print(f"XGBoost Regression R2 Score: {r2_xgb}")

# Step 8: Final Evaluation of Tuned Models
# Assuming 'rf_model' from previous step is the best model we have
best_rf_model = rf_model  # Assign the existing rf_model to best_rf_model
y_pred_best_rf = best_rf_model.predict(X_test)

# Define best_xgb_model, Assuming 'xgb_model' from previous step is the best XGBoost model
best_xgb_model = xgb_model # Assign the existing xgb_model to best_xgb_model
y_pred_best_xgb = best_xgb_model.predict(X_test)  # This line might also have similar issue if best_xgb_model is not defined

mse_best_rf = mean_squared_error(y_test, y_pred_best_rf)
r2_best_rf = r2_score(y_test, y_pred_best_rf)
print(f"Tuned Random Forest MSE: {mse_best_rf}")
print(f"Tuned Random Forest R2 Score: {r2_best_rf}")

mse_best_xgb = mean_squared_error(y_test, y_pred_best_xgb)
r2_best_xgb = r2_score(y_test, y_pred_best_xgb)
print(f"Tuned XGBoost MSE: {mse_best_xgb}")
print(f"Tuned XGBoost R2 Score: {r2_best_xgb}")

import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Residual Plot (Difference between true and predicted values)

# For Random Forest Model
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_rf, alpha=0.3, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2)
plt.title("Random Forest: True vs Predicted Delay Minutes")
plt.xlabel("True Delay Minutes")
plt.ylabel("Predicted Delay Minutes")
plt.show()

# For XGBoost Model
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_xgb, alpha=0.3, color='green')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2)
plt.title("XGBoost: True vs Predicted Delay Minutes")
plt.xlabel("True Delay Minutes")
plt.ylabel("Predicted Delay Minutes")
plt.show()

# Step 2: Distribution of Errors (Residuals)
rf_residuals = y_test - y_pred_rf
xgb_residuals = y_test - y_pred_xgb

plt.figure(figsize=(10, 6))
sns.histplot(rf_residuals, kde=True, color='blue', label='Random Forest', bins=50)
sns.histplot(xgb_residuals, kde=True, color='green', label='XGBoost', bins=50)
plt.title("Residual Distribution for Random Forest and XGBoost Models")
plt.xlabel("Residual (Error in Delay Minutes)")
plt.legend()
plt.show()

# Step 3: Feature Importance Plot (Random Forest and XGBoost)
# For Random Forest
importances_rf = rf_model.feature_importances_
features = X.columns

plt.figure(figsize=(10, 6))
sns.barplot(x=importances_rf, y=features, color='blue')
plt.title("Random Forest: Feature Importance")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

# For XGBoost
importances_xgb = xgb_model.feature_importances_

plt.figure(figsize=(10, 6))
sns.barplot(x=importances_xgb, y=features, color='green')
plt.title("XGBoost: Feature Importance")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()