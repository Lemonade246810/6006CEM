# -*- coding: utf-8 -*-
"""REGRESSION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1towHPKP6dzW-v4lqBCqHEtrPHYZWRDAb
"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset (limit to 100,000 rows)
url = "flight_delays.csv"
df = pd.read_csv(url, nrows=100000)  # Load only 100,000 rows
print(f"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns.")

# Display initial data overview
print("Dataset Overview:")
print(df.head())

print(df.info())

# Step 1: Data Preprocessing
# Handling missing values by dropping rows where 'DelayMinutes' is null (target variable)
df_clean = df.dropna(subset=['DelayMinutes'])

# Filling missing values for DelayReason
df_clean['DelayReason'].fillna('Unknown', inplace=True)

# Convert time-related columns into datetime format
time_columns = ['ScheduledDeparture', 'ActualDeparture', 'ScheduledArrival', 'ActualArrival']
for col in time_columns:
    df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')

# Feature engineering: Create delay features
df_clean['DepartureDelay'] = (df_clean['ActualDeparture'] - df_clean['ScheduledDeparture']).dt.total_seconds() / 60.0
df_clean['ArrivalDelay'] = (df_clean['ActualArrival'] - df_clean['ScheduledArrival']).dt.total_seconds() / 60.0

# Drop original time columns and other non-predictive columns
drop_columns = ['ScheduledDeparture', 'ActualDeparture', 'ScheduledArrival', 'ActualArrival',
                'FlightID', 'FlightNumber', 'TailNumber']
df_clean = df_clean.drop(columns=drop_columns)

# Convert categorical variables to numeric using Label Encoding
label_cols = ['Airline', 'Origin', 'Destination', 'DelayReason', 'AircraftType', 'Cancelled', 'Diverted']
label_encoders = {}
for col in label_cols:
    df_clean[col] = df_clean[col].astype(str)
    le = LabelEncoder()
    df_clean[col] = le.fit_transform(df_clean[col])
    label_encoders[col] = le

# Step 2: Define Features and Target
X = df_clean.drop(columns=['DelayMinutes'])  # Features
y = df_clean['DelayMinutes']  # Target

# Step 3: Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Subsample the training data for faster hyperparameter tuning
sample_indices = X_train.sample(frac=0.1, random_state=42).index
X_train_sample = X_train.loc[sample_indices]
y_train_sample = y_train.loc[sample_indices]

# Step 4: Hyperparameter Tuning
# Random Forest
rf_params = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
}
rf_model = RandomizedSearchCV(
    RandomForestRegressor(random_state=42),
    rf_params,
    n_iter=5,
    cv=3,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    random_state=42
)
rf_model.fit(X_train_sample, y_train_sample)
best_rf_model = rf_model.best_estimator_

# XGBoost
xgb_params = {
    'n_estimators': [100, 200],
    'max_depth': [3, 6],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8, 1.0],
}
xgb_model = RandomizedSearchCV(
    XGBRegressor(random_state=42),
    xgb_params,
    n_iter=5,
    cv=3,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    random_state=42
)
xgb_model.fit(X_train_sample, y_train_sample)
best_xgb_model = xgb_model.best_estimator_

# Step 5: Evaluate Models
# Random Forest
y_pred_rf = best_rf_model.predict(X_test)
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Random Forest Regression MSE: {mse_rf}")
print(f"Random Forest Regression R2 Score: {r2_rf}")

# XGBoost
y_pred_xgb = best_xgb_model.predict(X_test)
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f"XGBoost Regression MSE: {mse_xgb}")
print(f"XGBoost Regression R2 Score: {r2_xgb}")

# Step 6: Visualizations
# Residual Plots
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_rf, alpha=0.3, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2)
plt.title("Random Forest: True vs Predicted Delay Minutes")
plt.xlabel("True Delay Minutes")
plt.ylabel("Predicted Delay Minutes")
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_xgb, alpha=0.3, color='green')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2)
plt.title("XGBoost: True vs Predicted Delay Minutes")
plt.xlabel("True Delay Minutes")
plt.ylabel("Predicted Delay Minutes")
plt.show()

# Residual Distributions
rf_residuals = y_test - y_pred_rf
xgb_residuals = y_test - y_pred_xgb

plt.figure(figsize=(10, 6))
sns.histplot(rf_residuals, kde=True, color='blue', label='Random Forest', bins=50)
sns.histplot(xgb_residuals, kde=True, color='green', label='XGBoost', bins=50)
plt.title("Residual Distribution for Random Forest and XGBoost Models")
plt.xlabel("Residual (Error in Delay Minutes)")
plt.legend()
plt.show()

# Feature Importance
# Random Forest
importances_rf = best_rf_model.feature_importances_
features = X.columns

plt.figure(figsize=(10, 6))
sns.barplot(x=importances_rf, y=features, color='blue')
plt.title("Random Forest: Feature Importance")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

# XGBoost
importances_xgb = best_xgb_model.feature_importances_

plt.figure(figsize=(10, 6))
sns.barplot(x=importances_xgb, y=features, color='green')
plt.title("XGBoost: Feature Importance")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()